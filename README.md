# PhD Thesis
## *Expanding the Generative Space*: Data-Free Techniques for Active Divergence with Generative Neural Networks

This repo is the latex document of my PhD thesis that was completed in 2024 at Goldsmiths, University of London, under the supervision of Professor Mick Grierson and Professor Frederic Fol Leymarie. It was examined by Professor Phillippe Pasquier and Professor Marco Gillies on the 24th March 2025.

You can download the thesis on Goldsmiths Research Online: [https://research.gold.ac.uk/id/eprint/39039/](https://research.gold.ac.uk/id/eprint/39039/)
## Abstract 


Generative neural networks offer powerful tools for the generation of data in many domains, given their ability to model distributions of data and generate high-fidelity results. However, a major shortcoming is that they are unable to explicitly diverge from the training data in creative ways and are limited to fitting the target data distribution. This thesis presents a body of work investigating ways of training, fine-tuning, and configuring generative neural networks in inference in order to achieve data-divergent generation. This goal of configuring generative neural networks to diverge from their original training data or any existing data distribution is referred to as *active divergence*. All of the approaches presented in this thesis are data-free in their implementation, which inherently distinguishes these approaches from the traditional orthodoxy of imitation-based learning that is widespread throughout most machine learning research. The research presented in this thesis represents three categorical contributions to achieving active divergence: training without data, divergent fine-tuning, and network bending. In addition to this, a formal survey and taxonomy of active divergence methods is presented as another contribution of this thesis. The overriding goal of the research in this thesis is to *expand the generative space* of generative neural networks. All three methods presented achieve this, and point to a new approach to working with generative neural networks that does not rely on the imitation of, and derivation from data, for extracting its value and creative possibilities.

## Video Presentation

Following submission and examination of my PhD, I gave a talk at the UAL Creative Computing Institute as part of their postgraduate research talk series and the Computer AI & Data Science (CCI-CAIDS) monthly research seminar series on the 16th of April 2025. You can watch the talk by clicking on the embedded link below:

[![Still from PhD presentation talk](https://img.youtube.com/vi/mee3pyr1RwA/0.jpg)](https://www.youtube.com/watch?v=mee3pyr1RwA)


## Citation

```
@phdthesis{broad2024expanding,
  author = {Terence Broad},
  title = {Expanding the Generative Space: Data-Free Techniques for Active Divergence with Generative Neural Networks},
  year = {2024},
  school={Goldsmiths, University of London}
}
```
