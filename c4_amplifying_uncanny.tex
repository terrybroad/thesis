\chapter{Amplifying the Uncanny: Experiments in Divergent Fine-Tuning of Generative Neural Networks}
\label{ch:uncanny}

This chapter details several experiments in the divergent fine-tuning of pretrained models, away from the likelihood of modelling data towards generating novel, unseen data distributions. 
These experiments were the first documented attempts at doing this (to the best of my knowledge), concurrent and following approaches to divergent fine-tuning are detailed in \S \ref{survey:divergent}. 
The experiments were written up separately into two publications. 
The first, a short paper pre-print titled `Transforming the output of GANs by fine-tuning them with features from different datasets’ published on arXiv\footnote{
    This paper was rejected from the NeurIPS creativity workshop in favour of the paper that discussed the work done in the previous Chapter (\ref{ch:unstable_eq}) \citep{broad2019searching} and was never followed up for subsequent resubmission.} \citep{broad2019transforming}. 
The second, titled \textit{Amplifying The Uncanny} was published at the 8th Conference on Computation, Communication, Aesthetics \& X (xCoAx) \citep{broad2020amplifying}. 
The experiments of these two works have been combined into one chapter, as the first experiment directly inspired the second.

The focus of this chapter is primarily the work presented in the paper titled \textit{Amplifying The Uncanny}, and the visual outcomes that were presented as the series of works artworks \textit{Being Foiled}. 
The first experiment (\S \ref{c4:sec:first-exp}), although not achieving what I had originally set out to do, did lead to technical discoveries that led to the follow on experiments \S \ref{c4:sec:second-exp}, which were sucessful in achieving data divergent fine-tuning with notable visual and aesthetic outcomes.

\section{Motivation}

Following the work presented in Chapter \ref{ch:unstable_eq}, I was motivated to explore further the possibility of training generative neural networks without data. 
However, given the rather idiosyncratic nature of the arrangements of neural networks and loss functions presented in the last chapter, I wanted to pursue an approach that was more deliberate, with experiments that could be repeated by others more easily. 

Instead of training neural networks completely from random initialisation, I wanted find to new ways to fine-tune pre-trained models using novel loss functions and auxillary networks. 
The reasoning for this was that, it was clear, based on experiments such as deepdream (CITE) and tom whites perception engines (CITE), that CNN-based computer vision models had powerful representations baked into them, that could be optimised towards with gradient based optimisation in generative contexts.
My intuition was that if pretrained image recognition models could be used for generating individual images, then they also be used for fine-tuning existing generative models, in a data-divergent fashion.

Given the prohibitive amount of computational resources needed to train the then state-of-the-art (SOTA) models such as BigGAN (CITE) and StyleGAN (CITE) from scratch at the time, and the increasingly common practice of fine-tuning and transfer learning being used by creative people who wanted to work with high fidelity models (CITE). 
Instead of training from scratch, using pre-trained models to be fine-tuned in divergent ways was something that I could experiment much more rapidly. 
It also allowed for the alteration of existing learned features in a generator, to converge to a set of learned parameters for totally novel and divergent data distributions, in ways that diverge directly from the original training data (\S \ref{c4:sec:second-exp}). 

\section{Fine-Tuning with Learned Features from Other Data Domains}
\label{c4:sec:first-exp}

A generative model that is trained on a dataset contains within the weights information about the features that are present in the dataset domain. 
The learned parameters of the model then reflect the patterns present in the data domain. 
The thinking behind this experiment, was that, to be able to diverge from the original data domain that a model was trained on, features or information from another dataset could in some way be combined with the learned features from the existing domain, in order to make a new data distribution that combines features from multiple domains.

In this first experimental design, I attempted to perform some-kind of learned feature infusing into the weights of a pre-trained generative model, using an auxiliary classifier. 
The classifier was trained to discriminate between the original data domain, which in the case of these experiments was the ImageNet dataset, and a new custom domain that had a specific aesthetic quality\footnote{Training a new classifier from scratch does somewhat contradict the `data-free' narrative in this thesis. However, as this was a premilinary experiment, whose methods directly inspired the approach later taken in \S \ref{c4:sec:second-exp}, it was decided that documenting this work was an important part of the narrative of the thesis, whilst not claiming to be one of the major contributions of the overall thesis.}. 
The goal was to use this auxiliary classifier to infuse some of the learned features from the new domain into the generative model trained on the original domain by fine-tuning the generator to optimise towards generating these learned features. 

\subsection{Training the Data-Domain Classifiers}

Two data-domain classifiers were trained, both on custom datasets collected from the website Pinterest. Pinterest allows users to create and share image boards, collections of images that are based on a particular theme. 
Once an image is uploaded to Pinterest, it can be reused by any user and put into any board. 
As Pinterest is, in effect, a platform where people have put a lot of effort into grouping and categorising images into themes of all sorts of topics. 
Often the themes may be of a particular mood, aesthetic quality, or some other kind of esoteric theme. 
Making many of the image boards quite distinct in terms of their style and quality in comparison to what is normally collated and categorised into traditional image datasets for computer vision (circa 2008-2018), such as imagenet (CITE), which tend to be curated to have aesthetically`neutral’ images in them. (Expand on this in a footnote?)

\textbf{Figure 1: Samples from ImageNet}

Specific, recognisable aesthetic qualities, which were as distinct from the qualities of ImageNet images as I could find, were the goal for the domains that I wanted to try and infuse features from. 
I made two dataset by collating pinterest boards which had specific key-words in their titles. 
The first dataset was compiled from Pinterest board with ‘a e s t h e t i c’ in the title -- a phenomenon of 2010s internet culture of images with a particular aesthetic value, that is closely related to the `instagram aesthetic' (CITE?). 

\textbf{Figure 2: Samples from a e s t h e t i c dataset}

The second dataset was compiled from Pinterest board with ‘bleak’ in the title. 
These images are darker, higher contrast, and moodier. These images are again, in contrast with the ImageNet images. 
As well as that, they contrast strongly with the a e s t h e t i c images. 
This point of contrast between the datasets and classifiers was something that I intentionally made to have for reference when comparing the results visually later. 

\textbf{Figure 3: Samples from bleak dataset}

Both classifier models were trained using transfer learning from a pre-trained ResNet \citep{he2016deep} model that had been trained to classify ImageNet-ILSVRC-2012 1000 classes (Check this). 
Each classifier was trained for 450k iterations with a batch size of 30, half of each batch contained images from the respective Pinterest dataset and the other half images from the ImageNet dataset. 
The ‘a e s t h e t i c’ dataset was iterated through for approximately 500 epochs and the ‘bleak’ dataset for approximately 800 epochs. 
The training procedure for both models iterated through the ImageNet dataset for approximately 5 epochs. 
The training for both models was performed with the Adam optimiser [Kingma and Ba, 2014] with a learning rate of 0.0002 and with $\beta 1 = 0.5$ and $\beta 2 = 0.999$. 
These were used as the standard default training parameters for the ResNet implementation I had used (REF).

\subsection{Fine-Tuning Procedure}

The generator is fine-tuned towards maximising the probability of the class prediction for the auxiliary domain, which is either the aesthetic or bleak class. We fine-tuned the BigGAN model trained on ImageNet. The first initial experiment was done just maximising this on it’s own. \textbf{IS THIS ACTUALLY THE CASE? DO I HAVE ANY EVIDENCE OF ME DOING THIS?}

\textbf{Fig 4: Maximising just the classifier output}

In the second round of experiments the generator was optimised towards a weighted sum of both the auxiliary classifier and the frozen weights of the discriminator. 
The thinking behind this was that the discriminator should hold some kind of information about what is and isn’t a valid generated output from the original domain. 
My intuition was that optimising towards a weighted sum of these two models should allow the generator to optimise to an intersection of the two sets of features sourced from the original domain and the auxiliary domain.  

\textbf{Fig 5: Maximising weighted sum}

All of the experiments were performed for 500 training iterations, using the Adam optimiser \citep{kingma2015adam} with a learning rate of 0.00005 and with $\beta1 = 0$ and $\beta2 = 0.999$. 
Parameters which were left as the default training parameters from the BigGAN implementation I was using. 

% The results from training were somewhat unexpected. Training towards just maximising the classifier lead to ….. (RUN THESE experiments).

\textbf{Fig 6: Results just auxiliary classifier}

The results from training with both the classifier and the discriminator were definitely improved. 
The fine-tuned model retained more of the structural information from the original domain and the results generally are more intriguing. 
The results from the two domains differ from each other, but do not visibly resemble any of the information from their specific domains. 
Rather they are both quite similar in terms of visual appearance.

\textbf{Fig 7: Results classifier + discriminator}

The work from this original experiment shows that using frozen auxiliary models, it is very fast to adapt the weights of the model of a pre-trained generative model to something completely different. 
500 iterations on a 1080ti took about 5 minutes to achieve, which is dramatically faster than training a model from scratch, or even to perform traditional transfer learning with GANs which can still take many hours (even days) on consumer hardware. 

The original goal of this work was to see if you could transfer recognisable characteristics from one domain to another, which I was not able to achieve with this technique\footnote{Later work performing similar divergent fine-tuning with the frozen weights of CLIP has been able to achieve something similar to what I had originally intended in these experiments \citep{gal2022stylegan}, which is detailed further in \S \ref{survey:divergent}}. 
Whilst these initial experiments were not successful in achieving the original goal,these experiments are significant are for what it led to next. 
Using the frozen weights of the discriminator in fine-tuning was a hack I developed, in order get more diverse outputs from fine-tuning (\textbf{CHECK THIS}). 
Following these experiments, this got me thinking that the discriminator, such an important part of GAN training. The discriminator likely had powerful and unseen representations that could potentially be uncovered with minor modifications to this fine-tuning approach. 
The next section details follow up experiments, exploring these aspects of the discriminator's unseen visual representations.

\section{Fine-Tuning Towards `Unlikelihood'}
\label{c4:sec:second-exp}

The premise for this experiment was simple. In traditional GAN training, the generator is optimised towards generating what the discriminator predicts as being real data. 
But what if instead, it were optimised towards generating images that the discriminator predicts as being fake, what would happen?

\textbf{Figure 8a: GAN diagram 
Figure 8b: Fine-tuning towards fake diagram}

Akin to the previous experiment (\S \ref{c4:sec:first-exp}), the weights of the discriminator are kept frozen. 
There is a clear objective for what the discriminator would be learning in this case, and keeping the weights frozen allows us to interrogate a particular snapshot of the discriminator in training. 
Fine-tuning an existing generative model allows us to visually explore these representations directly.

The experiments were performed with the pretrained weights of the original StyleGAN trained on the FFHQ dataset on three precalculated checkpoints at three different images resolutions, that were a by-product of StyleGAN's progressively growing approach to training \citep{karras2017progressive}.
The FFHQ dataset was created by researchers at NVIDIA in an attempt to create a dataset of faces that was more diverse and less biassed towards white celebrities, in comparison to the other major dataset at the time for GAN training, which was the Celeb-A dataset. 
The parameters used for these three training runs can be seen in Table (\textbf{REF}).

\textbf{TABLE}

The first training run was performed on the snapshot of the model trained at 512x512. 
The fine-tuning process, just like the previous experiments, happened very quickly. 
Within 500 iterations the model had diverged significantly from the training data, and by 2000 iterations, the model had completely collapsed into one mode of generation. 

\textbf{Figure 9: Fine-tuning procedure}

By inspecting the loss function in training, it is clear that the fine-tuning process creates a feedback loop. 
The better the generator gets at generating images that the discriminator predicts as being fake, the higher the predicting for fake becomes. 
The gradients end up exploding, which then makes drastic changes to the weights of the generator and the network quickly collapses into a single abstraction for the whole data distribution. 

\textbf{Figure 9: Loss function from training
Figure 10: Batch of outputs at end of training}

I followed this with two further experiments, repeating the same procedure, but this time with the available weights for the StyleGAN model pre-trained at 256x256 and at 1024x1024. 

\textbf{Figure 11a: Fine-tuning training run for 256x256
Figure 11b: Fine-tuning training run for 1024x1024}

\subsection{Results}

Visually, the results from these experiments look quite distinct from the results of the model trained at 512x512. 
The 256x model progresses towards a paler, more washed out appearance. Whereas the 1024x model progresses towards harder, more rigid features. 
Seeing these experiments together reinforces the widely agreed understanding that the relationship between the generator and discriminator networks are in constant flux (CITE), and that the characteristics of what gives away an image as being fake that the discriminator is looking out for is constantly changing over the process of training.  

Both of these training runs follow the same feedback loop as before. 
Maximising the generation of outputs the discriminator perceives as being fakes quickly reinforces this and the loss function and gradients explode.

These experiments show that each one of these fine-tuning procedures is quite idiosyncratic. 
% While there is an agreed way of measuring how closely aligned a model is to a data distribution, that can be measured with log-likelihood, or an inception score. 
% There isn’t an agreed way of measuring how closely a model has diverged from data. 
% The experiments in this section are as close as you can get to directly maximising the unlikelihood of a data distribution, but that in itself has no universal metric or meaning. 
The various experiments show that the ‘unlikelihood’, as determined by the discriminator, is constantly evolving and relates closely to whatever state the generator and discriminator were in at the current time the checkpoint of the models is saved. 

These experiments are significant as it shows how quickly you can adapt the generated outcome of a model, just by inverting the loss function. 
Revealing aspects of the training procedure of GANs that are otherwise hidden from normal view. 
They also reveal to us aspects of the uncanny, with respect to how humans interpret images.
This training process can be used to directly interrogate the theory of the uncanny valley, which is expanded upon in the next section. 

\section{Relationship to The Uncanny}

If we revisit the results from the training 2B, and in particular focus on outputs from the model snapshot at 500 iterations, we can see that these images have some particularly striking qualities to them:

\textit{Figure 13: Batch output 500 itr}

The results are in all truthfulness quite horrifying. 
The first couple of people I showed these results two were horrified\footnote{
    The first person I showed them to was my partner at the time, whose response was to the effect of: ``Well that is horrifying, please never show me those pictures again''. I later showed the results to a PhD colleague of mine, Shringi Kumari, in the Goldsmiths iGGi office, who had an equally negative reaction.}
, and their repulsion to the images was so extreme that I did not show them to anyone else, including my supervisor, for another 6 months, as I had thought that I had created something genuinely bad. 
It was only when I later revisited these images that I realised the significance of these responses and how that related to notions of the uncanny. 

The uncanny is a psychological or aesthetic experience that can be characterised as observing something familiar that is encountered in an unsettling way. 
Jentsch defined the uncanny as an experience that stems from uncertainty, giving an example of it as being most pronounced when there is “doubt as to whether an apparently living being is animate and, conversely, doubt as to whether a lifeless object may not in fact be animate” (Jentsch 1906, 1997). 
This definition was later refined to argue that the uncanny occurs when something familiar is alienated, when the familiar is viewed in an unexpected or unfamiliar form (Freud 1919).

The uncanny valley is a concept first introduced in 1970 by Masahiro Mori, a professor of robotics. 
It describes how in the field of robotics, increasing human likeness increases feelings of familiarity up to a point (see Figure below), before suddenly decreasing. 
As representations of human or animal likeness approach a close resemblance to human or animal form, it provokes an unsettling feeling. 
Responses in likeness and familiarity rapidly become more negative than at any prior point. 
It is only when the robotic form is close to imperceptible with respect to human or animal likeness that the familiarity response becomes positive again (Mori, MacDorman, and Kageki 2012). 
As well as robotics, this phenomena has been observed in video games (CITE), visual effects (CITE), and animation (CITE). 
\textbf{GIVE EXAMPLES OF ARTISTS WHO DELIBERATELY EXPLOIT THE UNCANNY.}

\textit{Figure 14: Uncanny Valley diagram}

The original StyleGAN model trained on FFHQ, was one of the first generative models to be able to generate images that were completely indistinguishable from real people to the untrained eye (CITE). 
The website thispersondoesnotexist.com (CITE) and many examples of images from these models used to make fake social media accounts (CITE), demonstrate this model's ability to cross the threshold of the uncanny valley towards producing completely plausible and convincing images of people.

One of the novel aspects of these experiments is that we are able to observe a controllable and detailed crossing of this uncanny valley. 
Starting from realism, and training towards abstraction, the process crosses the uncanny valley in reverse. 
As the generator start to diverge from realism the images quickly become increasingly unsettling, before starting to plateau back to abstraction and returning to a more favourable likeness.

\textit{Figure 15: Training checkpoints + uncanny valley diagram?}

\textbf{ADD A FINAL PARAGRAPH HERE?}

\section{Discussion}

The two main experimental designs in this chapter showed that it is possible to fine-tune neural networks in a divergent fashion without any training data. 
Simply by freezing other models and using them for fine-tuning instead, it is possible to get very striking changes to the weights and outputs of the models very quickly. 
\cite{berns2020bridging} referred to this technique as loss hacking and on reflection that seems like an apt term. 
This work was very much experimental, using what pre-trained models were available to me at the time, which were unofficial implementations of BigGAN and StyleGAN. 
What is interesting about these models were that the weights of the discriminator were shared along with the generator. 
Something that was not done with the official model weights, or subsequent models. 
Training these models from scratch takes a huge amount of computational power, and would have been outside the realms of possibility given my resources. 
If I had needed to have trained these models myself to get access to the discriminator networks for experimentation, then these experiments would probably have never happened. 

Just like the work in the previous chapter, the training process for these experiments was extremely fast. 
Usually within 500 iterations, the most interesting changes had happened to the models, which took less than 5 minutes on modest hardware (one NVIDIA GTX 1080ti). 
This method, like the one previous, does not rely on large training datasets and large amounts of energy and computing resources to achieve notable outcomes. 


\section{Conclusion}

In this chapter I have demonstrated two distinct approaches to fine-tuning pre-trained generative neural networks in a data-divergent fashion.
These approaches are the first published methods for divergent finetuning (to the best of my knowledge).
A complete account of all known methods for divergent finetuning to date is given in \S \ref{survey:divergent}.
Whist this work is novel, the results from all of the training runs described here are very idiosyncratic. 
The results are contingent on the unique state that the auxiliary models are in when their parameters are saved into checkpoints during training. 
In the case of the discriminator, this is completely unpredictable and not repeatable. 
While this can make for surprising outcomes, it also means that the experiments described would be impossible to reproduce without the exact model checkpoints (and sequence of latents that are sampled in fine-tuning).

How would it be possible then to manipulate a generative network in way that was more controllable and repeatable?
This became a question that was playing on my mind after doing these experiments. 
The techniques described here use gradient descent to manipulate the weights of the model to produce novel outcomes. 
The process of gradient descent however, is not something that we as humans can clearly understand, or easily control (CITE).
I became preoccupied with finding a way of manipulating generative models, without relying on gradient descent. 
The next chapter is the third and final chapter that details a novel technical contribution of in this thesis, one that centre's humans in the creative process and allows them to manipulate generative neural networks without and training of fine-tuning whatsover.
