\chapter{Network Bending: Direct and Expressive Manipulation of Generative Neural Networks}
\label{ch:net_bend}

\section{Introduction}

This chapter details the work I undertook in developing the Network Bending framework, a way to directly manipulate the internet features of generative models during inference. This work was first published online as a pre-print on arxiv \citep{broad2020network}, then later a revised manuscript was accepted as a conference paper at EvoMUSART \citep{broad2021network}, and then as an extended journal paper in Entropy \citep{broad2022network}.
The experiments in the original paper were done for the task of image generation with StyleGAN2. 
A follow up study applying the same techniques to audio using a custom VAE trained on spectrograms (\S \ref{c5:sec:net-bend-audio}) was later done for the extended Entropy paper. 
In this chapter, these experiments are written chronologically.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{figures/c5_netbend/misc/network-bending-diagram.png}
    \caption[Visual overview of the \textit{network bending} framework]{Visual overview of the \textit{network bending} approach where deterministically controlled transformation layers can be inserted into a pre-trained network. As an example, a transformation layer that scales the activation maps by a factor of $k_x=k_y=0.6$ is applied (\S\ref{sec:affine}) to a set of features in layer 5 responsible for the generation of eyes, which has been discovered in an unsupervised fashion using our algorithm to cluster features based on the spatial similarity of their activation maps (\S \ref{c5:sec:clustering}). At the bottom left we show the sample generated by StyleGAN2 \citep{karras2019analyzing} trained on the FFHQ dataset without modification, while to its right we show the same sample generated with the scaling transform applied to the selected features. NB: the GAN network architecture diagram shown on the top row is for illustrative purpose only.}
    \label{fig:overview_diagram}
\end{figure}

\section{Motivation}

Following the experiments detailed in Chapters \ref{ch:unstable_eq} \& \ref{ch:uncanny} I wanted to find an approach for diverging from data was easier to control.
The inspiration for this came from a conversation with my supervisor Mick Grierson.
After showing him the results detailed in the previous chapters, he said that though he liked the results, he wanted a methods that was more tactile, saying something along the lines of `I just want to stick my hand in the model and squeeze it, and see what pops out the other side'  \citep{grierson2020personal}. 
This quote stuck with me and eventually let to the development of the framework described here.

In some of the early experiments that led to this work, I hard-coded simple transformations into StyleGAN1 \citep{karras2019style} models during inference.
These early experiments (which later went on to become the series of artworks \textit{Teratome} detailed in \S \ref{c7:subsubsec:teratome} led to the intuition that eventually led to implementation of many kinds of transformation layers (\S \ref{c5:sec:transforms}) and the clustering approach for grouping features together  (\S \ref{c5:sec:clustering}).
The motivation for developing the clustering algorithm was the observation that when transformations were applied to random subsets of convolutional filters in a layer, then in some instances, manipulation of groups of filters had apparently powerful semantic effects, that could not be captured my only manipulating individual filters, as was done in the approach presented by \citep{bau2019semantic}.

In creating this framework I wanted to give as much control and agency to people in order to manipulate models as possible. 
This flexibility of this framework was key in order to give as much control over to people, and in order the expand the generative space of deep generative models in a data divergent fashion.

\section{Transformation Layers}

\label{c5:sec:transforms}

I implemented a broad variety of deterministically controlled transformation layers that can be dynamically inserted into the computational graph of the generative model. 
The transformation layers are implemented natively in PyTorch \citep{paszke2019pytorch} for speed and efficiency. I
 treated the activation maps of each feature of the generative model as 1-channel images in the range -1 to 1. 
 Each transformation is applied to the activation maps individually before they are passed to the next layer of the network. 

 The transformation layers can be applied to all the features in a layer, or a random selection, or by using pre-defined groups automatically determined based on spatial similarity of the activation maps (Section \ref{c5:sec:clustering}). 
 Figure \ref{fig:c5:layerwide_comparison} shows a comparison of a selection of these transformations applied to all the features layer-wide in various layers of StyleGAN2.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/c5_netbend/misc/transform-comparison.png}
    \caption[Comparsion of various transformation layers applied in different layers of StyleGAN2]{A comparison of various transformation layers inserted and applied to all of the features in different layers in the StyleGAN2 network trained on the FFHQ dataset, showing how applying the same filters in different layers can make wide-ranging changes the generated output. The rotation transformation is applied by an angle $\theta=45$. The scale transformation is applied by a factor of $k_{x}=k_{y}=0.6$. The binary threshold transformation is applied with a threshold of $t=0.5$. The dilation transformation is applied with a structuring element with radius $r=2$ pixels.}
    \label{fig:c5:layerwide_comparison}
\end{figure}


\subsection{Pointwise Transformations}

I began with simple pointwise numerical transformations $f(x)$ that are applied to individual activation units $x$. 
I implemented four distinct numerical transformations: the first is \emph{ablation}, which can be interpreted as $f(x) = x \cdot 0$. 
The second is \emph{inversion}, which is implemented as $f(x) = 1 - x$. 
The third is \emph{multiplication by a scalar} $p$ implemented as $f(x) = x \cdot p$. 
The final transformation is \emph{binary thresholding} (often referred to  as posterisation) with threshold $t$, such that:
\begin{equation}
f(x) = \begin{cases}
    1,& \text{if } x\geq t\\
    0,              & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Affine Transformations}
\label{sec:affine}
For this set of transformations, each activation map $X$ for feature $f$ is treated as an individual matrix that simple affine transformations can be applied too. 
The first two are horizonal and vertical \emph{reflections} that are defined as:
\begin{equation}
X \begin{bmatrix}
-1 & 0 & 0\\
\ 0 & 1 & 0\\
\ 0 & 0 & 1
\end{bmatrix}\quad , \quad X \begin{bmatrix}
1 & \ 0 & 0\\
0 & -1 & 0\\
0 & \ 0 & 1
\end{bmatrix}
\end{equation}

\noindent The second is \emph{translations} by parameters $p_x$ and $p_y$ such that:
\begin{equation}
X \begin{bmatrix}
1 & 0 & p_x\\
0 & 1 & p_y\\
0 & 0 & 1
\end{bmatrix}
\end{equation}

\noindent The third is \emph{scaling} by parameters $k_x$ and $k_y$ such that:
\begin{equation}
X \begin{bmatrix}
k_x & 0 & 0\\
0 & k_y & 0\\
0 & 0 & 1
\end{bmatrix}
\end{equation}
Note that in this chapter, I only report on using uniform scalings, such that $k_x = k_y$. Finally, fourth is \emph{rotation} by an angle $\theta$ such that:
\begin{equation}
X \begin{bmatrix}
cos(\theta) & -sin(\theta) & 0\\
sin(\theta) & cos(\theta) & 0\\
0 & 0 & 1
\end{bmatrix}
\end{equation}

Other affine transformations can easily be implemented by designing the matrices accordingly.

\subsection{Morphological Transformations}

I implemented two of the possible basic mathematical morphological transformation layers, performing \emph{erosion} and \emph{dilation} \citep{soille1999erosion} when applied to the activation maps, which can be interpreted as 1-channel images (Figure \ref{fig:c5:morphological transforms}). 
These can be configured with the parameter $r$ which is the radius for a circular kernel (aka structural element) used in the morphological transformations.

\begin{figure}[!htb]
    \centering
    \subfloat[]{\label{subfig:morph-a}\includegraphics[width=.32\textwidth]{figures/c5_netbend/morphology_activations/original_007.png}}
    \hfill
    \subfloat[]{\label{subfig:morph-a-erode}\includegraphics[width=.32\textwidth]{figures/c5_netbend/morphology_activations/erode_007.png}}
    \hfill
    \subfloat[]{\label{subfig:morph-a-dilate}\includegraphics[width=.32\textwidth]{figures/c5_netbend/morphology_activations/dilate_007.png}}
    \caption[Examples of morphological transformations being applied to an individual activation map in Layer 10 of StyleGAN2]{Examples of morphological transformations being applied to an individual activation map in Layer 10 of StyleGAN2. (a) Unmodified activation maps. (b) Activation map after erosion was applied ($r=2$ pixels). (c) Activation maps after dilation was applied ($r=2$ pixels).}
    \label{fig:c5:morphological transforms}
 \end{figure}

\section{Clustering Features}
\label{c5:section:clustering}

As most of the layers in the current state of the art generative models, such as StyleGAN2, have very large numbers of convolutional features, controlling each one individually would be far too complicated to build a user interface around and to control these in a meaningful way. 
In addition, because of the redundancy existing in these models, manipulating individual features does not normally produce any kind of meaningful outcome. 
Therefore, it is necessary to find some way of grouping them into more manageable ensembles of sets of features. 
Ideally, such sets of features would correspond to the generation of distinct, semantically meaningful aspects of the image, and manipulating each set would correspond to the manipulation of specific semantic properties in the resulting generated sample. 
To achieve this, I developed a novel approach that combines metric learning and a clustering algorithm to group sets of features in each layer based on the spatial similarity of their activation maps. 
I trained a separate convolutional neural network (CNN) for each layer of the respective generative models (the StyleGAN2 generator and the decoder of our VAE) with a bottleneck architecture (first introduced by Gr{\'e}zl et al.~\citep{grezl2007probabilistic}) to learn a highly compressed feature representation; the later is then used in a metric learning approach in combination with the $k$-means clustering algorithm \citep{lloyd1982least, celebi2013comparative} to group sets of features in an unsupervised fashion. 

\subsection{Architecture}

For each layer of both generative models, I trained a separate CNN on the activation maps of all the convolutional features. 
As the resolution of the activation maps and number of features varies for the different layers of the model (a breakdown of which can be seen in Table \ref{tab:classifier-table}).
I employed an architecture that can dynamically be changed, by increasing the number of convolutional blocks, depending on what depth is required. 

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}%{llllll}
\hline
Layer & Resolution & \#features &  CNN depth & \#clusters & Batch size\\
\hline
1     & 8x8        & 512          & 1                & 5                & 500        \\
2     & 8x8        & 512          & 1                & 5                & 500        \\
3     & 16x16      & 512          & 2                & 5                & 500        \\
4     & 16x16      & 512          & 2                & 5                & 500        \\
5     & 32x32      & 512          & 3                & 5                & 500        \\
6     & 32x32      & 512          & 3                & 5                & 500        \\
7     & 64x64      & 512          & 4                & 5                & 200        \\
8     & 64x64      & 512          & 4                & 5                & 200         \\
9     & 128x128    & 256          & 5                & 4                & 80         \\
10    & 128x128    & 256          & 5                & 4                & 80         \\
11    & 256x256    & 128          & 6                & 4                & 50         \\
12    & 256x256    & 128          & 6                & 4                & 50         \\
13    & 512x512    & 64           & 7                & 3                & 20         \\
14    & 512x512    & 64           & 7                & 3                & 20         \\
15    & 1024x1024  & 32           & 8                & 3                & 10         \\
16    & 1024x1024  & 32           & 8                & 3                & 10       \\
\hline
\end{tabular}
\medskip
\caption[Table detailing model architecture for the ShuffleNet models used for clustering in StyleGAN2.]{
    \label{tab:classifier-table}Table showing resolution, number of features of each layer, the number of ShuffleNet \citep{zhang2018shufflenet} convolutional blocks for each CNN model used for metric learning, the number of clusters calculated for each layer using $k$-means and the batch size used for training the CNN classifiers for the StyleGAN2 models. Note: LSUN church and cat models have only 12 layers.
%\citep{lloyd1982least, celebi2013comparative}.
}
\end{table}

I employed the ShuffleNet architecture \citep{zhang2018shufflenet} for the convolutional blocks in the network, which is one of the state-of-the-art architectures for efficient inference (in terms of memory and speed) in computer vision applications. 
For each convolutional block, I utilised a feature depth of 50 and have one residual block per layer. 
The motivating factor in many of the decisions made for the architecture design was not focused on achieving the best accuracy per se. 
Instead, I wanted a network that can learn a sufficiently good metric while also being reasonably quick to train (with 12-16 separate classifiers required to be trained per StyleGAN2 model). 
I also wanted a lightweight enough network, such that it could be used in a real-time setting where clusters can quickly be calculated for an individual latent encoding, or it could be used efficiently when processing large batches of samples.

After the convolutional blocks, I flattened the final layer and learn from it a mapping into a narrow bottleneck $\vec{v} \in \mathbb{R}^{10}$, before re-expanding the dimensionality of the final layer to the number of convolutional features present in the layer of the respective generative model. 
The goal of this bottleneck is to force the network to learn a highly compressed representation of the different convolutional features in the generative model. 
While this invariably loses some information, most likely negatively affecting classification performance during training, this is in fact the desired result. 
I wanted to force the CNN to combine features of the activation maps with similar spatial characteristics so that they can easily be grouped by the clustering algorithm. 
Another motivating factor is that the chosen clustering algorithm ($k$-means) does not scale well for feature spaces with high dimensionality.

\subsection{Training}

I generated a training set of the activations of every feature for every layer of 1000 randomly sampled images and a test set of 100 samples for the models trained on all of the datasets used in our experiments. 
I trained each CNN using the softmax feature learning approach \citep{dosovitskiy2014discriminative}, a reliable method for distance metric learning. This method employs the standard softmax training regime \citep{bridle1990probabilistic} for CNN classifiers. 
Each classifier has been initialised with random weights and then trained for 100 epochs using the Adam optimiser \citep{kingma2015adam} with a learning rate of 0.0001 and with $\beta_1 = 0.9$ and $\beta_2 = 0.999$. 
All experiments were carried out on a single NVIDIA GTX 1080ti. The batch size used for training the classifiers for the various layers of StyleGAN2 can be seen in Table \ref{tab:classifier-table}. 
% The classifiers for the VAE were all trained with a batch size of 100.

After training, the softmax layer is discarded and the embedding of the bottleneck layer is used as the discriminative feature vector where the distances between points in feature space permit gauging the degree of similarity of two samples. 
This approach differs from standard softmax feature learning as it uses the feature vector from the bottleneck, rather than the last layer prior to softmax classification, giving a more compressed feature representation than the standard softmax feature learning approach.

\subsection{Clustering Algorithm}

\label{c5:sec:clustering}

Once each of the CNNs for every layer have been trained, they can then be used to extract feature representations of the activation maps of the different convolutional features corresponding to each layer of the generative model.
There are two approaches to this. The first is to perform the clustering on-the-fly for a specific latent for one sample. A user would want to do this to get customised control of a specific sample, such as a latent that has been found to produce the closest possible reproduction of a specific person from the StyleGAN2 model trained on the FFHQ dataset \citep{abdal2019image2stylegan,karras2019analyzing}. 
The second approach is to perform clustering based on an average of features' embedding drawn from many random samples, which can be used to find a general-purpose set of clusters.

The clustering algorithm for a single example is activated by a forward pass of the generative model performed without any additional transformation layers being inserted, to obtain the unmodified activation maps. 
The activation map $X_{df}$ for each layer $d$ and feature $f$ is fed into the CNN metric learning model for that layer $C_d$ to get the feature vector $\vec{v}_{df}$. 
The feature vectors for each layer are then aggregated and fed to the $k$-means clustering algorithm --- using Lloyd's method \citep{lloyd1982least} with Forgy initialization \citep{forgy1965cluster, celebi2013comparative}. 
This results in a pre-defined number of clusters for each layer. 
Sets of features for each layer can then be manipulated in tandem by the user.

Alternatively, to find a general purpose set of clusters, I first calculated the mean feature vector $\vec{\bar{v}}_{df}$ that describes the spatial activation map for each convolutional feature in each layer of generative model from a set of $N$ randomly generated samples --- the results in the paper are from processing 1000 samples. 
Then I performed the same clustering algorithm as previously for individual samples on the mean feature vectors. 
The number of clusters for each layer in StyleGAN2 can be seen in Table \ref{tab:classifier-table}. Examples form the clustering algorithm applied to the FFHQ StyleGAN2 model can be seen in Figure \ref{fig:c5:cluster_layer_comp_image}.

\begin{figure}[!htbp]
    \centering
        \subfloat[]{\includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/not_manipulated.png}}
        \hfill
        \subfloat[]{\includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_1_cluster0_mult-1.png}}
        \hfill
        \subfloat[]{\includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_3_cluster2_mult5.png}}
        \hfill
        \subfloat[]{\includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_5_cluster2_ablate.png}}
        \hfill
        \subfloat[]{ \includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_6_cluster4_dilate.png}}
        \hfill
        \subfloat[]{ \includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_9_cluster3_mult5.png}}
        \hfill
        \subfloat[]{\includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_10_cluster0_mult-1.png}}
        \hfill
        \subfloat[]{\includegraphics[width=.24\textwidth]{figures/c5_netbend/cluster_comparison/layer_15_cluster0_mult0.1.png}}
       \caption[A comparison of different transforms being applied to different clusters in various layers of StyleGAN2]{Examples from our clustering approach in the image domain. Clusters  of features in different layers of the model are responsible for the formation of different image attributes. (a) The unmanipulated result. (b) A cluster in layer 1 has been multiplied by a factor of -1 to completely remove the facial features. (c) A cluster in layer 3 has been multiplied by a factor of 5 to deform the spatial formation of the face. (d) A cluster in layer 6 has been ablated to remove the eyes. (e) A cluster in layer 6 has been dilated with a structuring element with radius $r=2$ pixels to enlarge the nose. (f) A cluster in layer 9 has been multiplied by a factor of 5 to distort the formation of textures and edges. (g) A cluster of features in layer 10 has been multiplied by a factor of -1 to invert the highlights on facial regions. (h) A cluster of features in layer 15 has been multiplied by a factor of 0.1 to desaturate the image. All transformations have been applied to sets of features discovered using our feature clustering algorithm (\S\ref{c5:section:clustering}) in the StyleGAN2 model trained on the FFHQ dataset.}
       \label{fig:c5:cluster_layer_comp_image}
    \end{figure}

The main motivation of the clustering algorithm presented in this paper was to simplify the parameter space in a way that allows for more meaningful and controllable manipulations whilst also enhancing the expressive possibilities afforded by interacting with the system. Our results show that the clustering algorithm is capable of discovering groups of features that correspond to the generation of different semantic aspects of the results, which can then be manipulated in tandem. These semantic properties are discovered in an unsupervised fashion, and are discovered across the entire hierarchy of features present in the generative model. Figure \ref{fig:c5:cluster_layer_comp_image} shows the manipulation of groups of features across a broad range of layers that control the generation of: the entire face, the spatial formation of facial features, the eyes, the nose, textures, facial highlights and overall image contrast.

\section{Manipulation Pipeline}

Transforms are specified in YAML (YAML Ain't Markup Language) configuration files \citep{ben2009yaml}, such that each transform is specified with 5 items: (i) the layer, (ii) the transform itself, (iii) the transform parameters, (iv) the layer type (i.e. how the features are selected in the layer: across all features in a layer, to pre-defined clusters, or to a random selection of features), and (v) the parameter associated with the layer type (either the cluster index, or the percentage of features the filter will randomly be applied to). 
There can be any number of transforms defined in such a configuration file.

After loading the configuration, the software either looks up which features are in the cluster index, or randomly apply indices based on the random threshold parameter. 
Then the latent is loaded, which can either be randomly generated, or be predefined in latent space $z$, or be calculated using a projection in latent space $w$ \citep{abdal2019image2stylegan,karras2019analyzing} (in the case of StyleGAN2). The latent code is provided to the generator network and inference is performed. 
As our implementation is using PyTorch \citep{paszke2019pytorch}, a dynamic neural network library, these transformation layers can therefore be inserted dynamically during inference as and when they are required, and applied only to the specified features as defined by the configuration. 
Once inference is unrolled, the generated output is returned. Figure \ref{fig:overview_diagram} provides a visual overview of the pipeline, as well as a comparison between a modified and unmodified generated sample.

\begin{figure}[!htb]
    \centering
    \subfloat[]{\label{subfig:layer-wide}\includegraphics[width=.24\textwidth]{figures/c5_netbend/layer-transform-types/layer-wide.png}}
    \hfill
    \subfloat[]{\label{subfig:random-layer}\includegraphics[width=.24\textwidth]{figures/c5_netbend/layer-transform-types/stochastic.png}}
    \hfill
    \subfloat[]{\label{subfig:cluster-layer}\includegraphics[width=.24\textwidth]{figures/c5_netbend/layer-transform-types/cluster.png}}
    \hfill
    \subfloat[]{\label{subfig:chaining-transformations}\includegraphics[width=.24\textwidth]{figures/c5_netbend/layer-transform-types/chaining-transforms.png}}
    \caption[Examples of transformation layers being applied to different configurations of features]{Examples of transformation layers being applied to different configurations of features in StyleGAN2. (a) Transformation applied layer wide. (b) Transformation applied to a random selection of filters in ony layer. (c) Transformation applied to a cluster in layer 2. (d) A combination of transformation layers applied to across a network, the configuration of transformations used to generate this image can be seen in Figure \ref{fig:c5:yaml-transform-config}.}
    \label{fig:c5:layer-transform-types}
 \end{figure}

 \begin{figure}[!htb]
 \begin{lstlisting}[language=yaml]
    ---
    transforms:
    - layer: 2
      transform: "invert"
      params: []
      features: "all"
      feature-param: 
    - layer: 6
      transform: "binary-thresh"
      params: [0.5]
      features: "random"
      feature-param: 0.5
    - layer: 6
      transform: "scalar-multiply"
      params: [5]
      features: "all"
      feature-param: 
    ---
    \end{lstlisting}
    \caption[Example YAML transformation config]{Example of a YAML transformation config that is used in the network bending framework. This config combines randomly applied layers and layer wide transformations. This config was used to generate the image Figure \ref{subfig:chaining-transformations}.}
    \label{fig:c5:yaml-transform-config}
 \end{figure}

\section{Network Bending in the Audio Domain}
\label{c5:sec:net-bend-audio}

As a follow up study to the original network bending approach on images, I applied the same approach to the audio domain as an extension to the work for the journal paper in Entropy \citep{broad2022network}. 
The motivation for this was to demonstrate that network bending was applicable to different types of media, to demonstrate the general purpose nature of this framework. 

For this study I trained a custom VAE model on spectrograms of music, and applied the exact same algorithm for clustering and applying the same transformation layers as was applied in network bending for image generation. 
Network bending has also been applied to audio by other researchers and practitioners, this efforts are detailed in \S \ref{c7:subsubsec:naotokui} and \S \ref{c7:subsubsec:ddsp}.

\subsection{Custom Audio Model}

For this experiment, I trained a variational autoencoder (VAE) \citep{kingma2013auto,rezende2014stochastic} on spectrograms extracted from a custom dataset of varied musical genres, totalling 3461 audio tracks. We base our approach on previous methods for learning generative models of spectrograms \citep{akten2018granma} and melspectrograms \citep{valenzuela2021melspecvae} with VAEs. The tracks are randomly split up into short sequences and the Fourier transform is performed with a hop size of 256 and a window size of 1024 to produce spectrograms that have a bin size of 513. The spectrograms are then cut into shorter sequences of a window length of 128. These shortened spectrograms are then converted to decibels and then normalised for training with the VAE.  

The VAE was built using a convolutional architecture with a latent vector with dimension $\vec{v} \in \mathbb{R}^{512}$ . The encoder has 5 layers that use standard convolutions with a kernel size of 5x5, a stride of 2x2 and no padding for all of the layers. The decoder uses transposed convolutions, table \ref{tab:c5:decoder-architecture} lists the output resolution, kernel size, stride, and padding parameters for each of the 5 convolutional layers. A fully connected layer is used in both the encoder and decoder to interface between the convolutional layers and the latent vector. The model was trained for 50 epochs on the dataset with batch normalisation using a batch size of 64. The model was trained using the Adam optimiser \citep{kingma2014adam} with a learning rate of 0.0003 and with $\beta_1 = 0$ and $\beta_2 = 0.99$.

After training it is possible to sample randomly in the latent space and then sample directly from the decoder. It is also possible to input audio sequences, both from the training set and outside of it, and produce reconstructions of the audio track mediated through the VAE model, in a method that we have previously referred to as \textit{autoencoding} \citep{broad2017autoencoding}. By performing this autoencoding procedure in combination with network bending, we can provide a new way of transforming and filtering audio sequences.

\begin{table*}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    Layer & Resolution & \#features & kernel size & stride & padding \\
    \hline
    1     & 8x33       & 512        & 5x5         & 1x2     & 0x2  \\
    2     & 17x65      & 256        & 3x5         & 2x2     & 2x2 \\
    3     & 32x129     & 128        & 4x5         & 2x2     & 2x2 \\
    4     & 64x257     & 64         & 4x5         & 2x2     & 2x2  \\
    5     & 128x513    & 1          & 4x5         & 2x2     & 2x2  \\
    \hline
    \end{tabular}
    \medskip
    \caption{\label{tab:c5:decoder-architecture}Table showing resolution, number of features of each layer, convolutional kernel size, strides, padding parameters for the decoder network in the spectrogram VAE.
    }
    \end{table*}

\subsection{Clustering}

the approach to clustering for this audio model was pretty much identical to what was done in \S \ref{c5:section:clustering}, with some minor modifications to the architecture of the ShuffleNet CNN models in order to process the non-square aspect ratio of the spectrograms (\textbf{check this?}). 
As the VAE model did not have as many layers as StyleGAN2, clusters were only calculated for four layers, the details of which can be seen in Table \ref{tab:c5:audio-clustering}.

\begin{table*}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    Layer & CNN Depth & \#clusters \\
    \hline
    1     & 1 &   5 \\
    2     & 2 &   5 \\
    3     & 3 &   4 \\
    4     & 4 &  4 \\
    \hline
    \end{tabular}
    \medskip
    \caption{\label{tab:c5:audio-clustering}Table showing the number of ShuffleNet \citep{zhang2018shufflenet} convolutional blocks for each CNN model used for metric learning, and the number of clusters calculated for each layer using $k$-means.
    }
    \end{table*}

\subsection{Results}

The clustering approach applied to the audio model appears to work well when visualising the spectrograms, and it is clear that this approach can capture and manipulate some semantically meaningful components in the audio signal\footnote{Unfortunately there was a bug in the decoding of the spectrograms back into audio which meant that the audio quality in the generated samples was very noisy -- something that I have not been able to fix.} (Figure \ref{fig:c5:cluster_audio_comp}). 
Not all of the transformations that can be applied to images work as well in audio, such as scaling and rotation.
This is not a surprise given that the location of each pixel is essential information used to represent frequence and time information in the audio signal, and can complete transform the information represented when manipulated.
However, the morphological transformations do at least preserve locality in the signal, and using these filters in generative models of spectrograms offers a completely new way to transform audio signals (\textbf{check this}).

\begin{figure}[tp!]
\vspace{-80pt}
\centering
    \subfloat[]{\includegraphics[width=.4\textwidth]{figures/c5_netbend/audio_clusters/soul_short_original.png}}
    \hfill
    \subfloat[]{\includegraphics[width=.4\textwidth]{figures/c5_netbend/audio_clusters/soul_short_reconstruction.png}}
   \hfill
    \subfloat[]{\includegraphics[width=.4\textwidth]{figures/c5_netbend/audio_clusters/layer_1_cluster3_ablate.png}}
    \hfill
    \subfloat[]{\includegraphics[width=.4\textwidth]{figures/c5_netbend/audio_clusters/layer_1_cluster3_mult_2.png}}
   \hfill
    \subfloat[]{\includegraphics[width=.4\textwidth]{figures/c5_netbend/audio_clusters/layer_3_cluster2_erode.png}}
    \hfill
    \subfloat[]{\includegraphics[width=.4\textwidth]{figures/c5_netbend/audio_clusters/layer_3_cluster2_dilate.png}}
   \caption[A comparison of different transforms being applied to different clusters in various layers of the SpectrogramVAE]{Examples from our clustering approach in the audio domain. (a) Spectrogram of an original source track not in the training set. (b) Reconstruction of source track using VAE without manipulation. (c) Reconstruction of the same signal where a cluster in layer 1 responsible for the generation of the transients of the signal has been ablated. (d) Reconstruction of the same signal where the same cluster in layer 1 responsible for the transients has been multiplied by a factor of 2, increasing the intensity of the transients in the resulting signal. (e) Reconstruction of the signal where a cluster in layer 3 responsible for the low and mid-range frequencies has been eroded with a structuring element with radius $r=2$ pixels, diminishing the intensity of these frequency components. (f) Reconstruction of the signal where the same cluster in layer 3 responsible for the low and mid-range frequencies has been dilated with a structuring element with radius $r=2$ pixels, increasing the intensity of these frequency components. The audio sample used is a clip from \textit{Saulsalita Soul} by Mr.RuiZ, reproduced and transformed with permission granted under the CC BY-NC 4.0 licence.}
   \label{fig:c5:cluster_audio_comp}
\end{figure}

\section{Discussion}

In this section, I discuss five perspectives: expressive manipulation, active divergence, comparisons of our results between the image and audio domains, comparisons with other methods, and finally show some real world examples where network bending has been used in the production of artworks.

\subsection{Expressive Manipulation}

The main motivation of the clustering algorithm presented in this chapter was to simplify the parameter space in a way that allows for more meaningful and controllable manipulations whilst also enhancing the expressive possibilities afforded by interacting with the system. 
These results show that the clustering algorithm is capable of discovering groups of features that correspond to the generation of different semantic aspects of the results, which can then be manipulated in tandem. 
These semantic properties are discovered in an unsupervised fashion and are discovered across the entire hierarchy of features present in the generative model. 
For example, Figure \ref{fig:c5:cluster_layer_comp_image} shows the manipulation of groups of features across a broad range of layers that control the generation of: the entire face, the spatial formation of facial features, the eyes, the nose, textures, facial highlights and overall image contrast. Figure \ref{fig:c5:cluster_audio_comp} shows our clustering algorithm performed in the audio domain, to demonstrate how aspects of the audio signal such as the transients and frequency components can be manipulated with various kinds of transformations.

Grouping and manipulating features in a semantically meaningful fashion is an important component of allowing expressive manipulation. 
However, artists are often also ready to consider surprising, unexpected results, to allow for the creation of new aesthetic styles, which can become uniquely associated with an individual or group of creators. 
Therefore the tool needs to allow for unpredictable as well as predictable possibilities, which can be used in an exploratory fashion and can be mastered through dedicated and prolonged use \citep{dobrian2006nime}. 
There is usually a balance between utility and expressiveness of a system \citep{jacobs2017supporting}. 
While it will be required to build an interface and perform user studies to more conclusively state that our approach has struck such a balance, our current results do show that both predictable semantic manipulation and more unpredictable, expressive outcomes are possible. 
This is a good indication that our approach represents a good initial step, and with further refinements, it can become an innovative powerful tool for producing expressive outcomes when using deep generative models. 

\subsection{Comparison Between Audio and Image Domains}

In this chapter I have demonstrated the network bending framework in both the image and audio domains. 
For the image domain, I have used StyleGAN2 \citep{karras2019analyzing}, the state of the art generative model for unconditional image generation, in the audio domain I have built our own custom generative model to demonstrate how the same principles of clustering features and applying transformations to clustered features can be applied indirectly to another domain. 
The generative model for audio I have presented is building on a much smaller body of research and has more room for improvement in terms of the fidelity of the generated outputs, however, it is still adequate and demonstrates that our clustering algorithm is capable of discovering semantically meaningful components of the signal (Figure \ref{fig:c5:cluster_audio_comp}). 
Some of the transformation layers that were designed for image-based models such as rotation and scaling do not transfer meaningfully into the audio domain. 
However, numerical and morphological transformations do work effectively in the audio domain, representing a completely new approach for manipulating audio signals. 

\subsection{Comparison with Other Methods}

With respect to the semantic analysis and manipulation of a generative model, our approach of clustering features and using a broad array of transformation layers is a significant advance over previous works \citep{Bau2017-vg,Bau2018-td,bau2019semantic, Brink2019-gc}. 
This recent thread of techniques only interrogate the function of individual features, and as such are unlikely to be capable of capturing a full account of how a deep network generates results since such networks tend to be robust to the transformation of individual features. 

The results in this chapter show that sets of features, which may not be particularly responsive to certain transformations, are very responsive to others. 
Figure \ref{fig:c5:ablation_comp} shows that in the model trained on the LSUN church dataset, a cluster of features, that when ablated has little noticeable effect on the result, can produce significant changes when using another transformation on the same cluster, here removing the trees and revealing the church building that was obscured by the foliage in the original result. 
This, I argue, shows that the functionality of features, or sets of features, cannot be understood only through ablation (which is the approach used in GAN dissection \citep{Bau2018-td}), because of the high levels of redundancy present in the learned network parameters. 
This shows that their functionality can be better understood by applying a wide range of deterministic transformations, of which different transformations are better suited to revealing the utility of different sets of features (Figures \ref{fig:c5:cluster_layer_comp_image} \& \ref{fig:c5:ablation_comp}).

\begin{figure}[!htbp]
   \centering
   \subfloat[]{\includegraphics[width=.32\textwidth]{figures/c5_netbend/church_ablation_comparison/not_manipulated.png}}
   \hfill
   \subfloat[]{\includegraphics[width=.32\textwidth]{figures/c5_netbend/church_ablation_comparison/layer_3_cluster1_ablate.png}}
   \hfill
   \subfloat[]{\includegraphics[width=.32\textwidth]{figures/c5_netbend/church_ablation_comparison/layer_3_cluster1_mult5.png}}
   \caption[A comparison of one cluster in StyleGAN2 having two different transforms applied to it]{Groups of features that are not particularly sensitive to ablation may be more sensitive to other kinds of transformation. (a) Original unmodified input. (b) A cluster of features in layer 3 that has been ablated. (c) The same cluster of features that has been multiplied by a scalar of 5. As can be seen, ablation had a negligible effect, only removing a small roof structure that was behind the foliage. On the other hand, multiplying by a factor of 5 removes the trees whilst altering the building structure to have gable roof sections on both the left and right sides of the church - which are now more prominent and take precedence in the generative process. Samples are taken from the StyleGAN2 model trained on the LSUN church dataset.}
   \label{fig:c5:ablation_comp}
\end{figure}

This method of analysis is completely \emph{unsupervised}, and does not rely on auxiliary models trained on large labelled datasets (such as in \citep{Bau2018-td, isola2017image, park2019semantic}) or other kinds of domain-specific knowledge. 
This approach therefore can be applied to any CNN based generative model architecture which has been trained on any dataset, as I have demonstrated by using the exact same clustering method for both image and audio domains. 
This is of particular relevance to artists who create their own datasets and would want to apply these techniques to models they have trained on their own data. 
Labelled datasets are prohibitively time consuming (and expensive) to produce for all but a few individuals or organisations. 
Having a method of analysis that is completely unsupervised and can be applied to unconditional generative models is important in opening up the possibility that such techniques become adopted more broadly.

The framework is the first approach to manipulating generative models that focuses on allowing for a large array of novel expressive outcomes. 
In contrast to other methods that manipulate deep generative models \citep{bau2019semantic, bau2020rewriting}, our approach allows the manipulation of any feature or set of features in any layer, with a much broader array of potential transformations. 
By allowing for the combination of many different transformations, it is evident that the outcomes can diverge significantly from the original training data, allowing for a much broader range of expressive outcomes and new aesthetic styles than would be possible with methods derived from semantic image synthesis \citep{isola2017image, chen2017photographic, park2019semantic} or semantic latent manipulation \citep{brock2016neural,  shen2020interpreting, harkonen2020ganspace}.

\section{Conclusion}

In this chapter, I have introduced a novel approach for the interaction with and manipulation of deep generative models, which has been demonstrated on generative models in the image and audio domains. 
By inserting deterministic filters inside pre-trained networks, this framework allows for manipulation to be performed inside the networks' black box and utilise it to generate samples that have no resemblance to the training data, or anything that could be created easily using conventional media editing software. 
This chapter also presents a novel clustering algorithm that can group sets of features, in an unsupervised fashion, based on the spatial similarity of their activation maps. 
I have demonstrated that this method is capable of finding sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated results in both image and audio domains. 
This provides a more manageable number of sets of features that a user could interact with. 

