\begin{abstract}

Generative neural networks offer powerful tools for generation of data in many domains, given their ability to model distributions of data and generate high-fidelity results. 
However, a major shortcoming is that they are unable to explicitly diverge from the training data in creative ways and are limited to fitting the target data distribution.
This thesis presents a body of work investigating ways of training, fine-tuning, and configuring generative neural networks in inference in order to achieve data-divergent generation.
This goal of configuring generative neural networks to diverge from their original training data, or any existing data distribution is referred to as \textit{active divergence}.
All of the approaches presented in this thesis are data-free, in their implementation, which inherintely distinguishes these approaches from the traditional orthodoxies of imitation-based learning, which is widespread throughout most machine learning research. 
The research presented in this thesis represents three categorical contributions to achieving active diverence. 
In addition to this, a formal survey and taxonomy of active divergence methods is presented as another contribution of this thesis. 
The goal of this thesis was to \textit{expand the generative space} of generative neural networks, and all three methods presented achieve this, and portend to a new approach to working with generative AI that does not rely on the imitation of, and derivation of data, for extracting its value and creative possibilties.


\end{abstract}