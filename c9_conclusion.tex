\chapter{Conclusion}
\label{ch:conclusion}

This thesis presents three novel approaches to training, fine-tuning, and intervening the the process of inference of generative neural networks, that allow for data-divergent generation, aka \textit{active divergence} (Ch. \ref{ch:active_div})).
All of the methods for achieving this are data-free, in their application, maning no data is used in technical intervention needed for achieving active divergence.
This distinction is an important one, as much of the research and development in generative AI increasingly relies on the increasing widespread use of data, often scraped from the web, without the consent of either the creators or publishing platforms.
This crisis of consent \citep{longpre2024consent}, and the major backlash against generative AI from communities of creative practitioners \citep{whiddington2022backlash}, provides clear evidence that finding ways of using generative AI that does not directly derive it's value from the aggregated efforts of human labour, even if done lawfully under the legal doctrines of `fair-use' \citep{sobel2017artificial,alhadeff2024limits} and `fair-dealing' \citep{guadamuz2023scanner}.

As well as legal arguments, there is both a moral and aesthetic argument, that we should be striving to move beyond simply faithful imitating data and replicating existing cultural capital with generative AI \citep{rafferty2016future}. 
Cultural production has largely stagnated into producing derivative rehashings of existing artist movements and styles \citep{fisher2009capitalist}, and this tendency has only been accelerated and entrenched by modern developments in generative AI.
With the work in this thesis, I have sought to find alternative ways in which generative AI can be used, and not using data in these methods has been key to moving beyond the orthodixies of generative AI and its derivation of value from existing cultural capital.

\section{Contributions}

In this section I will outline the four major contributions of this thesis, including three categorical contributions to methods for achieving active divergence, and finally a formal taxonomy of active divergence methods.

\subsection{Training without Data}

Chapter \ref{ch:unstable_eq} documents the first peer-reviewed and published apporach to training generative neural networks without data, one of the three categorical contributions to active diverence methods (Ch. \ref{survey:no-data}) presented in this thesis.
Whist this is not an approach that has been widely adopted by others, the series of artworks \textit{(un)stable equilibrium} that came from these experiments have been received well in the art world (\S \ref{c7:sec:unstable_eq}), winning the Grand Prize in the ICCV Compter Vision Art Gallery, and being exhibited internationally in arts festivals, and in both commerical and non-commerical art galleries. 

\subsection{Divergent Fine-Tuning}

Chapter \ref{ch:divergent} documents the first peer-reviewed and published approach to divergent fine-tuning of generative AI models.
This experiment went on the inform the intial definition of \textit{active divergence} \citep{berns2020bridging} and can be viewed as a categorical contribution to active divergence, an apporach that has had many other approaches to implementation (\S \ref{survey:divergent}).


\subsection{Network Bending}

Chapter \ref{ch:net_bend}, presents the network bending framework, and is the third categorical contribtion to active divergence methods presented in this thesis (\S \ref{survey:net_bend}).
Of the three chapter of original research this is the one that has had the most impact  (\S \ref{c7:sec:net-bend-artworks}; \S \ref{c7:sec:net-bend-impact}), being widely reused and adopted by many other artists and researchers, including inspiring the development of the next generation of StyleGAN models \citep{karras2021alias}.
In addition this is the approach that most successfuly achieves the main goal of this thesis, which is the \textit{expanding the generative space} of generative AI. 
Network bending provides a flexible, controllable, and general approach to intervening the in the computational process of inference in generative neural networks, and is the method that has the most scope for future research to build upon this method.

\subsection{Active Divergence Taxonomy}

The final contribution of this thesis is the survey and formal taxonomy of active divegence methods presented in Chapter \ref{ch:active_div}.
This survey presents a clear delineation and methods of active divergence, and of the eight categories outlined, examples of three of those categorical contributions were first published in the experiments detailed in Chapters \ref{ch:unstable_eq}, \ref{ch:divergent}  \& \ref{ch:net_bend}.

\section{Limitations}

Whilst three categorical contributions have been made to active divergence methods in this thesis, they have all primarly been demonstrated on feed-forward generative models for image generation.
StyleGAN \citep{karras2019style} and StyleGAN2 \citep{karras2019analyzing} were the primary models used in these experiments, and of the three experimental approaches, it is only Network Bending that has been demonstrated has been able to be generalised to domains beyond image generation and with other kinds of models (\S \ref{c7:sec:net-bend-impact}).

This work as relied heavily on aesthetic evaluation of these outputs of the generative systems in determining their value \ref{c8:sec:aesthetic}, and whilst this has been a valuable yard-stick in evaluating approaches that do not have clear means of quantitive evaluation, this does restrict the weight of the evaluation.
Instead I have relied on evaluating the impact of these works through their artistic reception and detailing how they have gone on to inspire other deveopments in research (Ch. \ref{ch:impact}), focusing heavily on the reuse of these techniques in assessing the impact of knowledge transfer from their dissemination \ref{c8:sec:generalisation}.
In the future research directions (\S \ref{c9:sec:future}), I will discuss possible ways that more formal evaluations of active divegence methods could be undertaken.

\section{Future Research Directions}
\label{c9:sec:future}

Here I will outline some future research directions that could be undertaken by others looking to further the contributions made in this thesis.

\subsection{Measuring and Evaluating Active Divergence}
As outlined in greater detail in \S \ref{c6:sec:future}, finding ways of measuring and evaluating active divergence methods is one potential fruitful area of research. 
This could take the form of both qualititative evaluation with human evaluators, or quantitative evaluation, possibly by reusing the existing extensive literature on distributional divergence in generative models \citep{gretton2019interpretable}.
A further area of research would be to evaluate how well quanitative measure of divergence align with human perception on divergence across distributions.

\subsection{Alternative Approaches to Active Divergence}
Also outlined in greater detail in \S \ref{c6:sec:future}, is the possibility of other methods for achieving active divergence. 
The taxonomy presented in \S \ref{c6:sec:taxonomy} is by no means exhaustive, and I anticipate there could be new approaches that would be substantive categorical contributions to these approaches. 
Open-ended reinforcement learning \ref{c6:subsec:open-ended}, and divergent RLHF \ref{c6:subsec:divergent-rlhf} are just two possible approaches that I have outlined. 
In addition, applying active divergence methods to autoregressive models, diffusion models, and the next generation of video generation models would be fruitful areas of research in my view.

\subsection{Improved Analysis and Manipuation of Models}

The method of analysis presented in Chapter \ref{ch:net_bend} for the network bending frameworks is not without it's flaws.
Training a separate model for each layer of the network is an expensive and time-consuming process, and makes network bending less accessible to artists with limited access to computational hardware who want to work with custom models.
\cite{oldfield2022panda,oldfield2024bilinear} have already improved in this approach by using tensor factorisation to analyse the feature maps of GANs and use that for downstream manipulation.
However analysing the appearance of feature maps still restricts these approaches to feed-forward convolutional generative models.
Alternative approaches like analysing influece functions \citep{koh2017understanding}, which has already been used successfully to analyse the importance of training data in large language models \citep{choe2024your}.
These approaches of model analysis could easily be adapted towards analysing a wider variety of models for expressive manipulations.

\subsection{Hacking the Next Generation of AI Models}

In the years that this research has been undertaken, a huge amount of development has taken place in the architectures and approaches to training generative models.
The majority of the research presented in this thesis was undertaken on GANs.
Whilst there have been some efforts to extend some of this research into denoising diffusion models \citep{dzwonczyk2024network} (\S \ref{c7:subsec:netbend-diffusion}). 
But there is still massive amounts of potential for hacking other kinds of generative models like transformer-based LLMs \citep{vaswani2017attention}, multi-modal LLMs \citep{zhang2024mm}, diffusion-transformer hybrid models \citep{peebles2023scalable} and video diffusion models \citep{ho2022video}.

\section{Summary}

This thesis has presented three categorical contributions to methods for active divergence: training without data (Ch . \ref{ch:unstable_eq}), divergent fine-tuning  (Ch . \ref{ch:divergent}), and network bending  (Ch . \ref{ch:net_bend}), all of which do not rely on any data in the process of their implementation. 
In addition to this, Chapter \ref{ch:active_div} presents a formal survey and taxonomy of active divegence methods.
The goal of this thesis was to \textit{expand the generative space} of generative neural networks, moving away from the paradigm of imitation based learning as being the only goal of configuring generative neural networks.
Of the three chapters of original work, the network bending framework is the one that has had the most impact as it has been widely reused by artists  (\S \ref{c7:sec:net-bend-artworks}) and other researchers  (\S \ref{c7:sec:net-bend-impact}). 
In addition this is the apporach that has most successfully expanded the generative space of generative models, with it's flexibility to application of models for different domains, architectures and ability to be used on model trained on any datasets.
The goal was to give back creative agency to human creators, and network bending is the primary means that this has been achieved in this thesis.