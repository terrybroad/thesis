\chapter{Conclusion}
\label{ch:conclusion}

This thesis presents three novel approaches to training, fine-tuning, and intervening in the process of inference with generative neural networks, that allow for data-divergent generation, aka \textit{active divergence} (Ch. \ref{ch:active_div}).
All of the methods for achieving this are data-free, meaning no data is used in the technical intervention needed for achieving active divergence.
This distinction is an important one, as much of the research and development in generative AI increasingly relies on the widespread use of data, often scraped from the web, without the consent of either the creators or publishing platforms.
This crisis of consent \citep{longpre2024consent}, and the major backlash against generative AI from communities of creative practitioners \citep{whiddington2022backlash}, provides clear evidence that finding ways of using generative AI that does not directly derive its value from the aggregated efforts of human labour (even if done lawfully under the legal doctrines of `fair-use' \citep{sobel2017artificial,alhadeff2024limits} and `fair-dealing' \citep{guadamuz2023scanner}) is an important direction of research.

As well as legal arguments, there is both a moral and aesthetic argument, that we should be striving to move beyond simply faithfully imitating data and replicating existing cultural capital with generative AI. 
\cite{rafferty2016future} argues that a lot of contemporary cultural production is simply replicating existing cultural capital without furthering it (an observation also made by Mark \cite{fisher2009capitalist}) and that this tendency has only been entrenched and codified by modern developments in generative AI.
With the work in this thesis, I have sought to find alternative ways in which generative AI can be used, and not using data in these methods has been key to moving beyond the orthodoxies of generative AI and its derivation of value from existing cultural capital.

\section{Contributions}

In this section, I will outline the four major contributions of this thesis, including three categorical contributions to methods for achieving active divergence, and finally a formal taxonomy of active divergence methods.

\subsection{Training without Data}

Chapter \ref{ch:unstable_eq} documents the first peer-reviewed and published approach to training generative neural networks without data, one of the three categorical contributions to active divergence methods (\S \ref{survey:nodata}) presented in this thesis.
Whilst this is not an approach that has been widely adopted by others, the series of artworks \textit{(un)stable equilibrium} that came from these experiments have been received well in the art world (\S \ref{c7:sec:unstable_eq}), winning the Grand Prize in the ICCV Compter Vision Art Gallery, and being exhibited internationally in arts festivals, and in both commercial and non-commercial art galleries. 

\subsection{Divergent Fine-Tuning}

Chapter \ref{ch:divergent} documents the first peer-reviewed and published approach to divergent fine-tuning of generative AI models.
This experiment went on the inform the initial definition of \textit{active divergence} \citep{berns2020bridging} and can be viewed as a categorical contribution to active divergence, an approach that has had many other approaches to implementation (\S \ref{survey:divergent}).


\subsection{Network Bending}

Chapter \ref{ch:net_bend}, presents the network bending framework and is the third categorical contribution to active divergence methods presented in this thesis (\S \ref{survey:bending}).
Of the three chapters of original research, this is the one that has had the most impact (\S \ref{c7:sec:net-bend-artworks}; \S \ref{c7:sec:net-bend-impact}), being widely reused and adopted by many other artists and researchers, including inspiring the development of the next generation of StyleGAN models \citep{karras2021alias}.
In addition, this is the approach that most successfully achieves the main goal of this thesis, which \textit{expanding the generative space} of generative AI. 
Network bending provides a flexible, controllable, and general approach to intervening in the computational process of inference in generative neural networks, and is the method that has the most scope for future research to build upon this method.

\subsection{Active Divergence Taxonomy}

The final contribution of this thesis is the survey and formal taxonomy of active divergence methods presented in Chapter \ref{ch:active_div}.
This survey presents a clear delineation and methods of active divergence, and of the eight categories outlined, examples of three of those categorical contributions were first published in the experiments detailed in Chapters \ref{ch:unstable_eq}, \ref{ch:divergent} \& \ref{ch:net_bend}.

\section{Limitations}
\label{c9:sec:limitations}

Whilst three categorical contributions have been made to active divergence methods in this thesis, they have all primarily been demonstrated on feed-forward generative models for image generation.
StyleGAN \citep{karras2019style} and StyleGAN2 \citep{karras2019analyzing} were the primary models used in these experiments, and of the three experimental approaches, only network bending has been demonstrated to be generalised to domains beyond image generation and with other kinds of models (\S \ref{c7:sec:net-bend-impact}).

This work has relied heavily on aesthetic evaluation of these outputs of the generative systems in determining their value \S \ref{c8:sec:aesthetic}, and whilst this has been a valuable yard-stick in assessing approaches that do not have clear means of quantitative evaluation, this does restrict the weight of the evaluation.
Instead, I have relied on evaluating the impact of these works through their artistic reception and detailing how they have gone on to inspire other developments in research (Ch. \ref{ch:impact}), focusing heavily on the reuse of these techniques in assessing the impact of knowledge transfer from their dissemination (\S \ref{c8:sec:generalisation}).
In the future research directions (\S \ref{c9:sec:future}), I will discuss possible ways that more formal evaluations of active divergence methods could be undertaken.

\section{Future Research Directions}
\label{c9:sec:future}

Here I will outline some future research directions that could be undertaken by others looking to further the contributions made in this thesis.

\subsection{Measuring and Evaluating Active Divergence}
As outlined in greater detail in \S \ref{c6:sec:future}, finding ways of measuring and evaluating active divergence methods is one potentially fruitful area of research.
This could take the form of both qualitative evaluation with human evaluators, or quantitative evaluation, possibly by reusing the existing extensive literature on distributional divergence in generative models \citep{gretton2019interpretable}.
A further area of research would be to evaluate how well quantitative measures of divergence align with human perception of divergence across distributions.

\subsection{Alternative Approaches to Active Divergence}
Also outlined in greater detail in \S \ref{c6:sec:future}, is the possibility of other methods for achieving active divergence. 
The taxonomy presented in \S \ref{c6:sec:taxonomy} is by no means exhaustive, and I anticipate there could be new approaches that would be substantive categorical contributions to these approaches. 
Open-ended reinforcement learning (\S \ref{c6:subsec:open-ended}), and divergent RLHF (\S \ref{c6:subsec:divergent-rlhf}) are just two possible approaches that I have outlined. 
In addition, applying active divergence methods to autoregressive models, diffusion models, and the next generation of video generation models would be fruitful areas of research in my view.

\subsection{Improved Analysis and Manipulation of Models}

The method of analysis presented in Chapter \ref{ch:net_bend} for the network bending frameworks is not without its flaws.
Training a separate model for each layer of the network is an expensive and time-consuming process, and makes network bending less accessible to artists with limited access to computational hardware who want to work with custom models.
\cite{oldfield2022panda,oldfield2024bilinear} have already improved in this approach by using tensor factorisation to analyse the feature maps of GANs and use that for downstream manipulation.
However, analysing the appearance of feature maps still restricts these approaches to feed-forward convolutional generative models.
Alternative approaches like analysing influence functions \citep{koh2017understanding}, which have already been used successfully to explore the importance of training data in large language models \citep{choe2024your}.
These approaches to model analysis could easily be adapted towards analysing a wider variety of models for expressive manipulations.

\subsection{Hacking the Next Generation of AI Models}

In the years that this research has been undertaken, a huge amount of development has taken place in the architectures and approaches to training generative models.
The majority of the research presented in this thesis was undertaken on GANs.
There have been some efforts to extend some of this research into denoising diffusion models \citep{dzwonczyk2024network} (\S \ref{c7:subsec:netbend-diffusion}). 
But there is still massive amounts of potential for hacking other kinds of generative models like transformer-based LLMs \citep{vaswani2017attention}, multi-modal LLMs \citep{zhang2024mm}, diffusion-transformer hybrid models \citep{peebles2023scalable} and video diffusion models \citep{ho2022video}.

\section{Summary}

This thesis has presented three categorical contributions to methods for active divergence: training without data (Ch. \ref{ch:unstable_eq}), divergent fine-tuning (Ch. \ref{ch:divergent}), and network bending (Ch. \ref{ch:net_bend}), all of which do not rely on any data in the process of their implementation. 
In addition to this, Chapter \ref{ch:active_div} presents a formal survey and taxonomy of active divergence methods.
Of the three chapters of the original work, the network bending framework is the one that has had the most impact as it has been widely reused by artists (\S \ref{c7:sec:net-bend-artworks}) and other researchers (\S \ref{c7:sec:net-bend-impact}). 
In addition, this is the approach that has most successfully expanded the generative space of generative models, with its flexibility to the application of models for different domains and architectures and its ability to be used on models trained on any dataset.
The goal of this thesis was to \textit{expand the generative space} of generative neural networks, and all three methods presented achieve this and point to a new approach to working with generative AI that does not rely on the imitation of, and derivation of data, for extracting its value and creative possibilities.
